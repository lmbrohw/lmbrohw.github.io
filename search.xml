<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>B1</title>
      <link href="/b1/"/>
      <url>/b1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读｜CycleGAN</title>
      <link href="/paperreading2/"/>
      <url>/paperreading2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>CycleGAN论文: <a href="https://arxiv.org/pdf/1703.10593.pdf">https://arxiv.org/pdf/1703.10593.pdf</a><br>原作者实现CycleGAN的Pytorch版本: <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p></blockquote><hr><div align="center"><strong><font size="5">理解CycleGAN模型和代码</font></strong></div><hr><h1 id="模型优点"><a href="#模型优点" class="headerlink" title="模型优点"></a>模型优点</h1><p>CycleGAN是一种基于生成对抗网络（GAN）发展而来的无监督机器学习方法。它是在pix2pix的基础上进行改进和扩展的。CycleGAN主要用于处理非配对图片的图像生成和转换任务，可以实现不同风格之间的转换。</p><p>与传统的GAN模型不同，CycleGAN的训练过程中没有配对的数据集。而是通过引入循环一致性损失来实现学习图像之间的映射关系。具体而言，CycleGAN包含两个生成器网络和两个判别器网络。其中，一个生成器负责将图像从源领域转换到目标领域，另一个生成器则相反；两个判别器负责判断生成的图像是否真实。通过交替训练生成器和判别器，CycleGAN能够学习到领域之间的映射规律。</p><p>CycleGAN广泛应用于图像风格转换任务。例如，它可以将照片转换为油画风格或将橘子转换为苹果，甚至可以进行马和斑马之间的图像转换。由于CycleGAN不需要成对的数据集，因此在数据准备上更加简单，具有很大的应用前景。</p><h1 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h1><p>CycleGAN由两个生成器和两个判别器构成</p><blockquote><p>G_AtoB() 是风格A向风格B的生成网络<br>G_BtoA() 是风格B向风格A的生成网络<br>dis_A() 是判别输入图片是否属于风格A的判别网络<br>dis_B() 是判别输入图片是否属于风格B的判别网络</p></blockquote><ol><li>G_AtoB()和G_BtoA()的输入为[B, C, W, H]，即batchsize, channels, width, height，输出与输入相同；</li><li>dis_A()和dis_B()的输入为[B, C, W, H]，即batchsize, channels, width, height，输出的维度是[B, 1]，网络中经过sigmoid函数输出，最后的取值范围在[0, 1]进行分类。</li></ol><blockquote><p>real_A 是从风格A的真实照片<br>real_B 是从风格B的真实照片<br>AtoB = G_AtoB(real_A) 是real_A经过生成网络转换得到的风格B的照片<br>BtoA = G_BtoA(real_B) 是real_B经过生成网络转换得到的风格A的照片</p></blockquote><p>下面是模型结构图</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/CycleGANModel.png" alt=""></p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/CycleGANModel2.png" alt=""></p><h1 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h1><p>生成器是由编码器、转换器和解码器组成的。</p><h2 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h2><p>编码器由三层卷积网络构成，假设编码器的输入为[1 3 256 256]，经过一层卷积层，变成[1 64 256 256]，经过第二层卷积层变成[1 128 128 128]，经过第三层卷积层变成[1 256 64 64]。</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>padding_mode<span class="token operator">=</span><span class="token string">'reflect'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>nn,Conv2d（输入通道数，输出通道数，卷积核大小， 步长，填充大小（默认填充0），填充模式=‘reflect’）其中采用的是InstanceNorm2d，并没有采用Normalization进行归一化。</p><p>Batch Normalization是指batchsize图片中的每一张图片的同一个通道一起进行Normalization操作。而Instance Normalization是指单张图片的单个通道单独进行Noramlization操作。</p><h2 id="转换器"><a href="#转换器" class="headerlink" title="转换器"></a>转换器</h2><p>本文转换器使用的是残差网络，<strong>残差网络</strong>目前大部分采用基于梯度的BP算法进行优化，该网络通常将输入信号向前传播，然后通过逆向传输误差值并利用梯度法更新参数。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/resnet.jpg" alt=""></p><p>残差网络除了减弱梯度消失外，还可以理解为这是一种自适应深度，也就是网络可以自己调节层数的深浅，至少可以退化为输入，不会变得更糟糕。可以使网络变得更深，更加的平滑，使深度神经网络的训练成为了可能。</p><p>原文中的描述是如果输入的图片大小是128x128就用6个残差块，如果图片大小是256x256就用9个残差块，残差网络的输入个输出大小一致，所以都是编码器的[1 256 64 64]</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ResidualBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>ResidualBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        block <span class="token operator">=</span> <span class="token punctuation">[</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span> padding_mode <span class="token operator">=</span> <span class="token string">'reflect'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span> padding_mode <span class="token operator">=</span> <span class="token string">'reflect'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        <span class="token punctuation">]</span>        self<span class="token punctuation">.</span>block <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>block<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>block<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>文中使用9个残差块</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            model <span class="token operator">+=</span> <span class="token punctuation">[</span>                ResidualBlock<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>解码器中用到的是反卷积（逆卷积）和卷积层，经过残差结构的tensor为[1 256 64 64]，经过第一层反卷积得到[1 128 128 128]经过第二层反卷积层得到[1 64 256 256]，再经过卷积层得到[1 3 256 256]，得到1张3通道的256x256的图片。</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>output_padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>padding_mode<span class="token operator">=</span><span class="token string">'reflect'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Tanh<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后经过Tanh映射到[-1 1]上，输出生成器重建的图像。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/GeneratorArgument.png" alt="" title="生成器网络参数信息表"></p><h1 id="判别器"><a href="#判别器" class="headerlink" title="判别器"></a>判别器</h1><p>判别器使用的是PatchGAN结构，普通的GAN判别器是将输入直接映射成一个表示输入样本是否为真样本的概率值，而PatchGAN是将输入映射为 N×N 的矩阵 X ，其中 X_ij 表示其中一个patch为真实样本的概率，然后将 X_ij 累加再求均值，即得到判别器的最终输出。</p><p>输入图像首先经过卷积核大小为4×4，步长为2，滤波器数量分别为64、128、256的三个卷积层，然后经过卷积核大小为4×4，步长为1，滤波器数量分别为512和1的两个卷积层，得到尺寸大小为30×30×1 的张量，最后经过均值池化层求平均值得到大小为 1×1×1 的张量，再通过调整该张量的维度，得到最终判断输入样本为真实样本的概率值 p。</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>InstanceNorm2d<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>LeakyReLU<span class="token punctuation">(</span><span class="token number">0.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>最后再经过：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/DiscriminatorArgument.png" alt="" title="判别器网络参数信息表"></p><h1 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h1><h2 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ImageDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> root <span class="token operator">=</span> arg<span class="token punctuation">.</span>train_dataroot<span class="token punctuation">,</span> unaligned<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>        transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>arg<span class="token punctuation">.</span>size<span class="token operator">*</span><span class="token number">1.2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> interpolation <span class="token operator">=</span> Image<span class="token punctuation">.</span>BICUBIC<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment">#调整输入图片的大小</span>        transforms<span class="token punctuation">.</span>RandomCrop<span class="token punctuation">(</span>arg<span class="token punctuation">.</span>size<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment">#随机裁剪</span>        transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token comment">#随机水平翻转图像</span>        transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token punctuation">]</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>unaligned <span class="token operator">=</span> unaligned        self<span class="token punctuation">.</span>files_A <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>root <span class="token operator">+</span> <span class="token punctuation">(</span>arg<span class="token punctuation">.</span>train_dataroot<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'2'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'/*.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>files_B <span class="token operator">=</span> <span class="token builtin">sorted</span><span class="token punctuation">(</span>glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>root <span class="token operator">+</span> <span class="token punctuation">(</span>arg<span class="token punctuation">.</span>train_dataroot<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'/'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'2'</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'/*.jpg'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        item_A <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_A<span class="token punctuation">[</span>index <span class="token operator">%</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_A<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>unaligned<span class="token punctuation">:</span>            item_B <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_B<span class="token punctuation">[</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_B<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            item_B <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>Image<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_B<span class="token punctuation">[</span>index <span class="token operator">%</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_B<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>convert<span class="token punctuation">(</span><span class="token string">'RGB'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">{</span><span class="token string">'A'</span><span class="token punctuation">:</span> item_A<span class="token punctuation">,</span> <span class="token string">'B'</span><span class="token punctuation">:</span> item_B<span class="token punctuation">}</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_A<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>files_B<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">train_data_loader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train_data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ImageDataset<span class="token punctuation">(</span>unaligned<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>batch_size<span class="token operator">=</span>arg<span class="token punctuation">.</span>batchSize<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> train_data_loader<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>训练数据集文件的存放要求：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token operator">-</span>A2B<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>train<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>A<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>A_images<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>B<span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span><span class="token operator">-</span>B_images<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>通过调用train_data_loader()函数得到字典格式的数据，可以通过data[‘A’]，和data[‘B’]操作将不同类型的图片取出来。</p><p>其中的图片还会经过：</p><blockquote><p>调整图片大小至[1.2size 1.2size]<br>随机裁减至[size size]大小<br>随机水平反转<br>归一化</p></blockquote><h2 id="生成图片缓冲区"><a href="#生成图片缓冲区" class="headerlink" title="生成图片缓冲区"></a>生成图片缓冲区</h2><p>为了使训练模型更加稳定，实验设置了一个图像缓冲区，用来存储在训练过程中生成器生成的动漫风格图像。在单独训练判别器时，随机混合加载缓冲区里面的动漫风格图像和当前轮生成器最新生成的图像。该缓冲区的大小由变量max_size控制，本实验中默认设置为50。通过传入这种不同时间生成的动漫风格图像给判别器进行训练，可以极大地提升模型训练的稳定性，保证损失函数的值不会大幅度的上下波动。</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ReplayBuffer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> max_size<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> <span class="token punctuation">(</span>max_size <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'Empty buffer or trying to create a black hole. Be careful.'</span>        self<span class="token punctuation">.</span>max_size <span class="token operator">=</span> max_size        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>            <span class="token keyword">def</span> <span class="token function">push_and_pop</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>        to_return <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> element <span class="token keyword">in</span> data<span class="token punctuation">.</span>data<span class="token punctuation">:</span>            element <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>element<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">)</span> <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>max_size<span class="token punctuation">:</span>                self<span class="token punctuation">.</span>data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>element<span class="token punctuation">)</span>                to_return<span class="token punctuation">.</span>append<span class="token punctuation">(</span>element<span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                <span class="token keyword">if</span> random<span class="token punctuation">.</span>uniform<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0.5</span><span class="token punctuation">:</span>                    i <span class="token operator">=</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>max_size<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>                    data_index<span class="token punctuation">.</span>append<span class="token punctuation">(</span>i<span class="token punctuation">)</span>                    to_return<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>                    self<span class="token punctuation">.</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> element                <span class="token keyword">else</span><span class="token punctuation">:</span>                    to_return<span class="token punctuation">.</span>append<span class="token punctuation">(</span>element<span class="token punctuation">)</span>        <span class="token keyword">return</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>to_return<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>缓冲区的数据初始化大小为50，当缓冲区没有图片的时候，我们把输入的data写入缓冲区，并且返回输入图片，当缓冲区满的时候，50%的可能会随机更新缓冲区数据，将新的数据放进来，替换掉之前生成的数据，之前的数据返回，也会有50%的可能直接返回输入的data数据。</p><h2 id="更新学习率"><a href="#更新学习率" class="headerlink" title="更新学习率"></a>更新学习率</h2><p>学习率初始为0.0002，总的epoch为200，在0-100的时候，学习率为0.0002，在100-200的时候，学习率逐渐线性减小为0，所以需要进行学习率的更新。</p><p>pytorch中提供了torch.optim.lr_scheduler.LambdaLR（）函数，其中的学习率衰减需要自己编写函数设定。</p><p>利用python实现为：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyLambdaLR</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_epochs<span class="token punctuation">,</span> offset<span class="token punctuation">,</span> decay_start_epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>n_epochs <span class="token operator">=</span> n_epochs        self<span class="token punctuation">.</span>offset <span class="token operator">=</span> offset        self<span class="token punctuation">.</span>decay_start_epoch <span class="token operator">=</span> decay_start_epoch    <span class="token keyword">def</span> <span class="token function">step</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">1.0</span> <span class="token operator">-</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> epoch <span class="token operator">+</span> self<span class="token punctuation">.</span>offset <span class="token operator">-</span> self<span class="token punctuation">.</span>decay_start_epoch<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_epochs <span class="token operator">-</span> self<span class="token punctuation">.</span>decay_start_epoch<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h2><p>在第一次训练的时候对模型中参数进行初始化。学习率的初始值为0.0002，模型会迭代训练200个epoch，因此，我们考虑在第0-100个epoch期间，学习率为0.0002，在训练达到第100个epoch时，学习率将开始线性减小，直到减小为0。</p><p>参数初始化代码：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">weights_init_normal</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>    classname <span class="token operator">=</span> m<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__    <span class="token keyword">if</span> classname<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'Conv'</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span>    <span class="token keyword">elif</span> classname<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'BatchNorm2d'</span><span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span>        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">0.02</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h2><p>在训练中得到的结果都是tensor，如何由张量得到图片进行存储和查看，也是十分重要。下面的代码使gpu上的 -1～1 之间的数据转化为0-255之间的值。</p><p>具体代码实现细节如下</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">TensorToImage</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">:</span>    real_image <span class="token operator">=</span> <span class="token number">255</span><span class="token operator">*</span><span class="token punctuation">(</span>T<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.5</span> <span class="token operator">+</span> <span class="token number">0.5</span><span class="token punctuation">)</span>    real_image <span class="token operator">=</span> real_image<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>    real_image <span class="token operator">=</span> real_image<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> real_image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cyclegan </tag>
            
            <tag> 风格迁移 </tag>
            
            <tag> image-to-image translation </tag>
            
            <tag> 图像处理 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文翻译｜CycleGAN</title>
      <link href="/paperreading1/"/>
      <url>/paperreading1/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文原文 <a href="https://ieeexplore.ieee.org/document/8237506">https://ieeexplore.ieee.org/document/8237506</a><br>arxiv版本 <a href="https://arxiv.org/pdf/1703.10593.pdf">https://arxiv.org/pdf/1703.10593.pdf</a></p></blockquote><hr><div align="center"><strong><font size="5">使用循环一致的对抗网络进行未配对图像到图像的转换</font></strong></div><hr><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/cover.png" alt="" title="给定任意两个无序的图像集X和Y，我们的算法学习自动将一张图片从一种风格转换到另一种风格，反之亦然。示例应用（底部）：使用一位著名艺术家的画集，学习到将用户的照片以这样的风格展示出来"></p><h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1 摘要"></a>1 摘要</h1><p>图像到图像的转换是一类关于视觉和图像的问题，这类问题的目的是通过使用成对的图片作为训练集，学习到输入图像和输出图像的映射关系。然而，对于很多任务来说，成对的训练数据是无法得到的。我们提出了一种在没有成对的数据的情况下学习将图像从源域X转换到目标域Y的方法。我们的目标是通过使用一种对抗损失函数学习一种映射G:X→Y，使得判别器无法分辨出图像G(X)和图像Y。由于该映射受到巨大的限制，因此我们为映射G搭配一个相反的映射F:Y→X，同时加入一个循环一致性损失函数来确保F(G(X)) ≈ X（反之亦然）。在不存在成对训练数据的情况下，我们比较了风格迁移、物品变形、季节转换、照片增强等几个任务的定性结果，都证明了我们的方法较之前几种方法的优越性。</p><h1 id="2-介绍"><a href="#2-介绍" class="headerlink" title="2 介绍"></a>2 介绍</h1><p>在1873年某一个晴朗的春日，当Claude Monet把他的绘画架放置在Argenteuil附近的塞纳河畔，他看到了什么？如果彩色照片在当时就已被发明的话，或许就能记录下这清澈的河流以及倒映在河流里的蓝天，Monet通过他纤细的画笔和明亮的调色板将这一场景给传达出来了。如果Monet画画的事情发生在 Cassis 小港口的一个凉爽的夏夜，那么会发生什么？漫步在一个挂满Monet画作的画廊里，我们可以轻易地想象到他会在画作中怎样描绘这些场景：或许是在淡雅的夜色中添上令人惊艳的一笔，或是变化平缓的光影范围。</p><p>我们可以想象这些所有的东西，尽管从未见过Monet画作中的对应真实景象的照片。取而代之的是，我们已经了解过Monet画作和风景照片，我们可以推断出这两者的差异，同时据此想象出从一种风格转换到另一种风格的样子。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/2.png" alt="" title="配对的训练数据（左）组成训练示例{x_{i},y_i }_(i=1)^N，其中yi与给定的每一个xi对应。与之不同的是我们用未配对的训练数据（右）组成源集{x_i }_(i=1)^N∈X和目标集{y_i }_(i=1)^M∈Y，其中没有提供关于哪一个xi匹配哪一个yi的信息"></p><p>在这篇文章中，我们提出了一个能够学习做相同的事情的系统：在缺乏成对训练图像的情况，捕捉一张图像数据集的特征，并弄清楚怎样把这些特征转换到其他图像数据集上。</p><p>这个问题可以描述成广义的图像到图像的转换，从给定的场景x完成一张图像到另一个场景y的转换。举例：灰度图片到彩色图片，图像到语义标签，轮廓到图片。发展了多年的计算机视觉、图像处理和图形学学界提出了强大的有监督翻译系统，它需要成对的数据{x,y}。然而，获取成对的训练数据是非常困难且昂贵的。例如，只有几对用于语义分割的数据集，并且它们很小。在艺术风格化这样的图像任务中，获取输入输出图像对会更困难，因为所需的输出图像非常复杂，特别是艺术创作。对于许多任务而言，就像物体变形这一类任务的输出更加不容易定义。</p><p>因此我们寻找一种可以在没有成对输入输出示例情况进行两种域之间转换的算法，我们假设这两个域之间有某种特定的关系。例如，它们是同一特定场景的两种不同渲染风格，我们来探索学习这种关系。尽管缺少成对数据的监督样本，我们仍然可以在集合层面使用监督学习：在给出X域图像和另一组不同的Y域图像，我们可以训练出一个映射G：X → Y 使得输出y=G(x)，x∈X，与图像y难以被判别器区分开，该判别器被训练用来区分生成样本yi和真实样本y。理论上，这一项将包括符合经验分布PY(y)的y’的输出分布（通常这要求G是随机的）。因此有一个最佳的映射G将X域转换为与Y域有相同分布的Y’域。然而，这样的转换不能保证独立分布的输入x和输出y在某种意义上配对——这儿有很多的映射G可以导出与y’一样的分布。不止如此，实际上，我们发现单独优化判别器是困难的：标准程序通常会导致出现众所周知的模式崩溃问题，即所有不同的输入图像都映射成相同的输出图像并且优化无法继续。</p><p>为了解决这个问题，我们在我们的模型中添加了更多的结构。因此，我们利用转换需要具有“循环一致性”的属性，在某种意义上，如果我们翻译英语到法语，然后又将法语翻译回英语，我们应该能够回到原始的那个英语句子。从数学上讲，如果我们有一个函数G：X→Y和一个函数F：Y→X，函数G和函数F能够相互颠倒，那么这两个函数映射是双射。我们将这个结构应用到映射G和F的同步训练中，并且加入一个循环一致性损失函数以保证F(G(x))≈x和G(F(y))≈y。合并这个损失函数与在X域Y域的对抗损失函数就能实现非成对的图像到图像转换的目标。</p><p>我们将这个方法应用到更广的领域上，包括风格迁移、物体变形、属性迁移和图像增强。我们将其与之前的方法比较，以前的方法依赖人工定义风格内容，或者共享的内部参数，并且我们这个方法优于这些baseline，我们在<a href="https://github.com/junyanz/CycleGAN">https://github.com/junyanz/CycleGAN</a> 提供我们的代码，在<a href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</a> 处查看论文的所有版本</p><h1 id="3-相关工作"><a href="#3-相关工作" class="headerlink" title="3 相关工作"></a>3 相关工作</h1><p>对抗生成网络在图像生成、图像编辑和表征学习等领域取得了瞩目的成就。近段时间的几种条件图像生成方法也采用了相同的思路。例如文本转图像、图像修复和视频预测等其他领域，比如视频和三维数据。对抗生成网络成功的关键是：对抗损失函数促使生成的图像在原则上与真实图像无法区分开。这对于图像生成任务特别有用，图像生成正是很多计算机图像任务致力于优化的目标。我们采用对抗损失函数来学习一种这样的映射关系，以致于转换得到图像与目标域的图像难以区分。</p><p>图像到图像的转换的想法至少可以追溯到Hertzmann等人的图像类比，该模型在一对输入输出的训练图像上采用一个无参的纹理模型。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/3.png" alt="" title="我们的模型包含两个映射函数G：X→Y和F：Y→X，相关的对抗判别器DY和DX。DY使G将X转换到与Y域无法区分的输出，反过来对于DX，F和X同样适用，为了进一步规范化映射函数，我们引入了两个“循环一致性损失”来确保如果我们将图像从一个域转换到其它域，然后再转换回来，我们应该得到最开始的图像：（b）前向循环一致性损失：x→G(x) →F(G(x)) ≈x，（c）后向循环一致性损失：y→F(y) →G(F(y)) ≈y"></p><p>近段时间的更多方法则使用一个输入输出样例数据集训练卷积神经网络。我们的研究建立在Isola等人的pix2pix框架上，这个框架使用条件对抗生成网络学习输入到输出的映射。类似的想法也应用在诸如从轮廓、图像属性、布局语义生成图片这类任务中，然而，与之前工作不同的是，我们在没有成对训练样例情况下学习映射关系。</p><p>其他几个旨在关联两个数据域X和Y的方法也解决了不成对数据的问题。最近，Rosales等人提出了一个贝叶斯框架，通过对原图像以及从多风格图像中得到的似然项进行计算，得到一个基于区块、基于先验信息的马尔可夫随机场。更近一些的研究中，CoupledGANs和跨膜态场景网络则使用一种权重共享策略来学习跨领域的通用表达。与我们这个方法同时期的研究，有Liu等人将变分自编码和对抗生成网络相结合来扩展网络模型。同时期的另一个研究尝试让输入和输出共享特定的“内容”特征，即使他们所属不同风格。他们也使用对抗网络并添加一些项目来促使输出在预先定义的度量空间内更接近于输入。例如标签分类空间、图片像素空间和图像特征空间。</p><p>不同于以上方法，我们的设计不依赖任何特定任务和预定义输入输出似然函数，也不要求输入和输出在一个相同的低纬度嵌入空间。因此我们的方法对于许多视觉和图像任务是一个通用的解决方案。我们直接在5.1节将本方案与先前的方法进行比较。与我们工作同时期的，在相同的程序中，Yi等人受到机器翻译中双重学习的启发，单独给未配对的图像到图像的转换引入一个相似的目标。</p><p>将可传递性(transitivity)作为一种结构数据规范化的方式由来已久，几十年来，在视觉追踪(visual tracking)任务中，确保简单的前后向传播一致 (simple forward-backward consistency) 已经成为一个标准。在语言处理领域，“反向翻译与核对（back translation and reconsiliation）”是翻译员用来验证并提高翻译质量的一种技术，机器翻译也是如此。最近，高阶循环一致性已经被用于动作检测、三维目标匹配、协同分割、稠密语义分割校准和景物深度估计。其中，Zhou和Godard等人的工作与我们最为相似。他们使用循环一致性损失作为一种用可传递性监督CNN训练的方式。在这工作中，我们将引入类似的损失来让生成器G和F彼此保持一致。</p><p>神经风格迁移是另一种优化图像到图像转换的方法，通过将一张图像的内容与另一图像的风格基于匹配预训练期的深度特征的伽马矩阵统计信息相结合，从而合成一幅新的图像。另一方面，我们主要关注的是：通过尝试刻画更高层级外观结构之间的对应关系，学习两种图像风格域之间的映射关系，而不是两张特定的图片。因此，我们的方法能适用于其他任务，例如绘画转图片，物体变形等单个样品转换方法表现不好的地方。我们在5.2节比较了这两种方法。</p><h1 id="4-方法"><a href="#4-方法" class="headerlink" title="4 方法"></a>4 方法</h1><p>我们的目标是学习两个给定训练样本中数据域X与Y之间的映射函数，其中：</p><script type="math/tex; mode=display">\begin{array}{ll}\left\{x_{i}\right\}_{i=1}^{N} & x_{i} \in X, x \sim p_{\text {data }}(x) \\\left\{y_{i}\right\}_{i-1}^{M} & y_{i} \in Y, y \sim p_{\text {data }}(y)\end{array}</script><p>如图3所示，我们的方法包含了两个映射函数：</p><script type="math/tex; mode=display">\begin{array}{ll}\left\{x_{i}\right\}_{i=1}^{N} & x_{i} \in X, x \sim p_{\text {data }}(x) \\\left\{y_{i}\right\}_{i-1}^{M} & y_{i} \in Y, y \sim p_{\text {data }}(y)\end{array}</script><p>另外，我们引入了两个对抗性判别器DX和DY：DX旨在区分{x} 与转换后的图像 {F(y)}；同样，DY用于区分{y} 与 转换后的图像{G(x)}。我们构建的损失模型包含两部分：对抗损失使生成的图像的分布与目标域的图像的数据分布相匹配；循环一致性损失防止学习到的映射G与F相互矛盾。</p><h2 id="4-1-对抗损失"><a href="#4-1-对抗损失" class="headerlink" title="4.1 对抗损失"></a>4.1 对抗损失</h2><p>我们为两个映射函数都应用了对抗损失，对于映射函数G：X→Y和它的判别器DY，我们有如下的表达式：</p><script type="math/tex; mode=display">\begin{array}{ll}\left\{x_{i}\right\}_{i=1}^{N} & x_{i} \in X, x \sim p_{\text {data }}(x) \\\left\{y_{i}\right\}_{i-1}^{M} & y_{i} \in Y, y \sim p_{\text {data }}(y)\end{array}</script><p>当映射G尝试生成与数据域Y相似的图像G(x)时，判别器DY致力于区分转换得到的G(x)和真正的样例图像y。我们为映射函数F：Y→X和判别器DX引入一个类似的对抗损失函数L_GAN (G,D_X,Y,X)。</p><h2 id="4-2-循环一致性损失"><a href="#4-2-循环一致性损失" class="headerlink" title="4.2 循环一致性损失"></a>4.2 循环一致性损失</h2><p>理论上对抗训练可以学习到映射G和F，并分别生成与目标域Y和X同分布的输出（严格地讲，这要求映射G与F是一个随机函数）。然而在容量较大情况下，网络可以将同一组输入图像映射到目标域中任何随机的图像排列，同时目标域中任何学习到的映射都可以产生与目标分布相匹配的输出分布。为了进一步减少可能的映射函数的空间，我们认为学习到的映射函数应该具有循环一致性，对于域X中的每一张图像x，该图像转换循环应该能够让图像x转换回到原始图像：x→G(x) →F(G(x)) ≈x,我们称之为前向循环一致性。类似地，对于Y域中的每一张图像y，生成器G和F也应该满足后向循环一致性：y→F(y) →G(F(y)) ≈y我们可以使用一个循环一致性损失来激励这种行为：</p><script type="math/tex; mode=display">\begin{aligned}\operatorname{Loss}_{\text {cycle }}=\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|F(G(x))-x\|_{1}\right] \\&+\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G(F(y))-y\|_{1}\right] .\end{aligned}</script><p>在预备实验中，我们也尝试用F(G(x)) 与x 之间、G(F(Y)) 与y 之间的对抗损失代替损失中的L1 范数，但是没有观察到更好的性能。在arXiv版本中可以观察到由循环一致性损失引起的行为。</p><h2 id="4-3-完整的模型"><a href="#4-3-完整的模型" class="headerlink" title="4.3 完整的模型"></a>4.3 完整的模型</h2><p>我们的完整的模型对象是：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}\left(G, F, D_{X}, D_{Y}\right)=& \mathcal{L}_{\mathrm{GAN}}\left(G, D_{Y}, X, Y\right) \\&+\mathcal{L}_{\mathrm{GAN}}\left(F, D_{X}, Y, X\right) \\+\lambda \mathcal{L}_{\mathrm{cyc}}(G, F)\end{aligned}</script><p>对象的相对重要性。我们致力于解决:</p><script type="math/tex; mode=display">G^{*}, F^{*}=\arg \min _{G, F} \max _{D_{x}, D_{Y}} \mathcal{L}\left(G, F, D_{X}, D_{Y}\right)</script><p>法能被看作训练两个 “自动编码器”：</p><script type="math/tex; mode=display">\begin{aligned}&F \circ G: X \rightarrow X \\&G \circ F: Y \rightarrow Y\end{aligned}</script><p>然而，每一个自动编码器都有特殊的内部结构：它们通过中间介质将图片映射到自身，这个中间介质是图像的另一个域的转换。这种配置也可以被视为一个“对抗性自动编码器”的特例，它使用对抗损失来训练自动编码器的瓶颈层，以匹配任意的目标分布。在我们的例子里，X→Y自动编码器的目标分布是Y域分布。在5.1.3节，我们将我们的方法与消去了完整对象的模型进行比较，经验表明，这两个对象（损失函数）在获得高质量结果上起关键作用。</p><h1 id="5-实现"><a href="#5-实现" class="headerlink" title="5 实现"></a>5 实现</h1><p>我们采用了J.Johnson文章中的生成网络架构，J.Johnson等人在神经风格转换和超分辨率方面取得了令人印象深刻的成果。这个网络架构包含两个步长为2的卷积层，几个残差模块和两个1/2步长的卷积层。与Johnson的方法类似，我们使用了正则化，对于判别网络，我们使用70×70的PatchGANs，PatchGANs致力于辨别70×70的重叠图像块的真假。这种补丁级的判别器结构比完整的图像判别器拥有更少的参数，并且可以以一种完全卷积的方式被应用于任意尺寸的图像。</p><p>我们将最近研究工作中的两种技术应用到我们的模型中以稳定模型训练。第一，对于LGAN，我们用最小二乘损失替代负对数似然损失。这个损失函数在训练和生成更高质量的结果期间表现得更稳定。等式1就变为：</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\text {LSGAN }}\left(G, D_{Y}, X, Y\right) &=\mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\left(D_{Y}(y)-1\right)^{2}\right] \\&+\mathbb{E}_{x \sim p_{\text {data }}(x)}\left[D_{Y}(G(x))^{2}\right]\end{aligned}</script><p>第二，为了减小模型训练时的震荡，我们遵循Shrivastava 等人的策略，用生成图像的历史而不是最新生成网络产生的图像来更新判别器DX和DY。我们使用一个图像缓存器来存储最近生成的50张图片。</p><p>请参阅我们的arXiv论文来获取更多的关于数据集、体系结构和训练程序的细节。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/4.png" alt="" title="不同方法在城市景观数据上训练的标记→照片的映射，从左至右：输入, BiGAN/ALI, CoGAN, CycleGAN(本方法),在成对数据上训练的pix2pix，以及真实图片"></p><h1 id="6-结果"><a href="#6-结果" class="headerlink" title="6 结果"></a>6 结果</h1><p>我们首先将我们的方法与最近的关于使用成对数据集完成未配对的图像到图像的转换方法做比较，输入输出对可用于评估。然后我们研究了对抗损失和循环一致性损失的重要性，并将我们的方法与方法的几种变体进行比较。最后，在不存在成对数据情况下，我们在更大的应用范围内证明了算法的通用性。为了简洁，我们将我们的方法称为CycleGAN。</p><h2 id="6-1-评估"><a href="#6-1-评估" class="headerlink" title="6.1 评估"></a>6.1 评估</h2><p>我们使用与“pix2pix”相同的评估数据集，并且将我们的方法与几个基线（baseline）进行定量与定性的比较。我们也对全损失函数进行了消融研究。</p><h3 id="6-1-1-Baseline"><a href="#6-1-1-Baseline" class="headerlink" title="6.1.1 Baseline"></a>6.1.1 Baseline</h3><p><strong>CoGAN</strong> 这个方法学习到一个生成X域数据和Y域数据的GAN生成器，前几层进行权重绑定，并共享对数据的潜在表达。通过找到一个生成图像X的潜在表达，然后将这个潜在表达以Y域风格呈现，可以实现从X到Y的风格转换。</p><p><strong>Pixel loss+GAN</strong> 就像我们的方法一样，Shrivastava用一个对抗损失来训练X到Y的转换，正则项‖X−Y‖1用来纠正像素层级上过大的变动。</p><p><strong>Feature loss+GAN</strong> 我们也测试了，使用预训练网络（VGG-16，relu4_2）在图像深度特征上计算L1损失，而不是在RGB像素值上计算。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/6.png" alt="" title="在地图↔航拍图片测试的AMT“real vs fake” "></p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/7.png" alt="" title="在城市景观标注图片→照片的评估中得到的不同方法的FCN得分"></p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/8.png" alt="" title="不同方法在城市景观图照片→标注的分类性能"></p><p><strong>BiGAN/ALI</strong> 无条件约束GAN训练了一个生成器G：Z→X，将随机噪声Z映射到图像X，BiGAN和ALI提议学习一个逆向映射函数F：X→Z，即使他们最初的设计是为了将一个潜在的向量z映射到图像x，我们实现了相同的目标，将原始图像x映射到目标图像y。</p><p><strong>pix2pix</strong> 我们也同在成对数据上训练的pix2pix做了比较，为了看看在没有使用成对数据情况我们能够与这个上限有多接近。<br>为了公平的比较，我们采用了同我们的方法相同的架构和细节（除了Co GAN）来实现所有的基线模型。由于架构上的差异，我们使用了CoGAN的公共的实现版本。</p><h3 id="6-1-2-与Baseline对比"><a href="#6-1-2-与Baseline对比" class="headerlink" title="6.1.2 与Baseline对比"></a>6.1.2 与Baseline对比</h3><p>如图4图5所示，我们不能在任何基线模型上取得令人信服的结果。另一方面，我们的方法通常能够产生与完全监督的pix2pix具有同等质量的转换结果。我们排除了图像中的pixel loss+GAN和feature loss+GAN，因为这两种方法都无法产生接近目标域的结果。</p><p>此外，我们的方法与基线模型通过三种方式进行了定量比较。首先，我们在AMT工人上进行“真对假”的研究来评估主观现实主义。第二，我们在城市景观数据上训练图片→标签的任务，将输出标签图像与城市景观的标准指标得到的真实图像进行比较。最后，我们在相同的数据集上训练标签→图片任务，用一个现成的全卷积语义分割网络评估输出的图片。我们发现，在这三个实验中，我们的方法明显优于基线模型。表1展示了在AMT感知现实主义任务中的表现。我们看到我们的方法能够在大约四分之一的地图→图片和图片→地图的实验中骗过参与者，而几乎所有的基线模型都不能做到如此。表2和表3评估了在城市景观图像的标注→图片任务的性能，我们的方法在这两种情况下都优于基线模型。每个实验的详细过程和结果可以在我们的arXiv版本中找到。</p><h3 id="6-1-3-消融研究"><a href="#6-1-3-消融研究" class="headerlink" title="6.1.3 消融研究"></a>6.1.3 消融研究</h3><p>我们对全损失的消融进行了比较，图6展示了几个定量示例。移除对抗损失或者循环一致性损失都会大大降低结果质量。因此我们认为这两项对我们实验的结果都至关重要。我们也在单向循环损失上评估了我们的方法：</p><script type="math/tex; mode=display">\begin{aligned}&\mathrm{GAN}+\operatorname{loss} \mathbb{E}_{x \sim p_{\text {data }}(x)}\left[\|F(G(x))-x\|_{1}\right] \\&\mathrm{GAN}+\operatorname{loss} \mathbb{E}_{y \sim p_{\text {data }}(y)}\left[\|G(F(y))-y\|_{1}\right]\end{aligned}</script><p>我们发现它通常会导致训练不稳定，并导致模式崩溃，尤其是对于被移除的映射方向。我们也定量测了城市景观照片→标签任务上的消融，该测量结果可以在我们的论文arXiv版本中找到。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/9.png" alt="" title="在城市景观上训练的标签↔图片映射方法的不同变体。从左至右：输入，单独的循环一致性损失，单独的对抗损失，GAN+前向循环一致性损失，GAN+后向循环一致性损失，CycleGAN（本方法）和真实图像。单独的循环一致性损失和GAN+后向循环一致性损失不能产生与目标域相似的图像。单独的对抗损失和GAN+前向循环一致性损失出现模式崩溃。无论输入图片是什么都能产生完全相同的标签映射"></p><h2 id="6-2-应用"><a href="#6-2-应用" class="headerlink" title="6.2 应用"></a>6.2 应用</h2><p>我们在不存在成对训练数据情况下演示了我们的方法的几种应用，我们观察到在训练数据上的转换比在测试数据上通常更有吸引力。应用在所有训练数据和测试数据的完整结果可以在我们项目的网站上看到。</p><p>这个模型被训练用来将Imagenet上的一类物品转换到另一类（每一类包含大约1000张训练图像）。Turmukhambetov提出了一个子空间模型来将一个物品转换到另一个相同类别的物品，但是我们的方法致力于两个视觉上相似但类别不同的物品之间的变形。</p><p>这个模型在从Flickr下载的Yosemite风景照上进行训练。</p><p>我们用从Flickr和WikiArt下载的风景图片来训练这个模型，注意，与最近的“神经风格迁移”工作不同的是，我们的方法学习仿造艺术作品集，而不是迁移一件艺术作品的风格。在5.2节我们对结果做了对比。</p><p>从画作中生成照片 对于画作→照片，我们发现引入一个额外损失使映射能保持输入输出间的色彩成分是有用的。特别地，在采用了Taigman的技术后，当真实的目标域示例被提供用作生成器的输入，规范化生成器以接近恒等映射。</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{G A N}(G, F) &=\mathbb{E}_{y \sim p_{\text {dait }}}\left[\|G(y)-y\|_{1}\right] \\&+\mathbb{E}_{x \sim p_{\text {data }}}\left[\|F(x)-x\|_{1}\right]\end{aligned}</script><p>在不使用Lidentity时，生成器G和F可以在对色调没有要求时自由地改变输入图像的色调。例如，当学习Monet画作和Flickr照片之间的映射时，生成器经常将白天的画作映射到黄昏时拍的照片，因为这样的映射在对抗损失和循环一致性损失上可能同等有效。身份映射损失的影响可以在arXiv论文中找到。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/10.png" alt="" title="几个图像转换问题的结果，这些图片是相对成功的结果，有关更全面的结果，请访问我们的网站"></p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/11.png" alt="" title="我们将输入的图像转换成不同的艺术风格，请访问我们的网站获取更多示例"></p><p>在图9，我们展示了将Monet画作转换为照片的结果，这些图片显示了被包括在训练集中的画作的转换结果，而本文中的其他实验，我们仅仅评估和展示测试集结果。因为训练集不包括成对数据，所以为训练集提供看似合理的转换结果是一项重要的任务。确实，自从Monet不再创作新的画作时，泛化看不到“测试集”的原画作不是一个迫切的问题。</p><p>我们展示了我们的方法可以被用来生成景深较浅的图片上。我们在从Flickr下载的花朵照片上训练这个模型。源域由智能手机拍摄的花朵的照片组成，这些照片由于拍摄光圈小通常具有很深的景深。目标照片由具有大光圈的单反相机拍摄得到。我们的模型成功地将智能手机拍摄的照片生成为具有较浅景深的图片。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/12.png" alt="" title="Monet画作映射成照片风格的结果，请访问我们的网站获取更多示例"></p><h1 id="7-局限与讨论"><a href="#7-局限与讨论" class="headerlink" title="7 局限与讨论"></a>7 局限与讨论</h1><p>虽然我们的方法在许多情况下可以取得令人信服的结果，但这些结果并不会一直都这样好。图12中展示了几个典型的失败案例，在涉及颜色和纹理变形的任务上，与上面报告中提到的许多任务一样，我们的方法通常能够成功。我们也探索了要求几何变换的任务，但是收效甚微。例如，在猫→狗转换任务中，对风格转换的学习退化为对输入图像进行最小程度的改变。处理更多和更极端的变换，特别是几何变换，这是未来工作的重点问题。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/13.png" alt="" title="照片增强：从一组iPhone照片映射到专业的DSLR照片，系统通常会学习生成短焦点。这里我们展示了测试集中最成功的一些结果——平均性能表现要比这个差得多。请查看我们的网站来了解更全面和随机的示例"></p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/14.png" alt="" title="将我们的方法与neural style transfer进行了比较。从左至右：输入图像，使用单个代表性图像作为风格图像的结果，使用目标域所有图像的结果，以及CycleGAN"></p><p>训练集的分布特征也会导致一些案例失败。例如，图12中马→斑马转换任务就因为我们的模型只在ImageNet训练，而不包括人类骑马或斑马的图片而导致失败。</p><p>我们也发现在成对训练数据和非成对训练数据的结果之间存在无法消弭的差距。在一些案例中，这种差距似乎很难甚至不可能消除掉：例如，我们的方法在照片→标签任务的输出中有时会排列树和建筑的标签。要解决这种歧义，可能需要某种形式的弱语义监督。集成的弱监督和半监督数据也许能够形成更强大的转换工具，其成本仍然只是完全监督系统的一小部分。</p><p>尽管如此，在多数情况下，使用非成对数据训练依然是可行的，并且我们应该加以利用。在这种“无监督”的情况下，这篇论文拓展了其可能使用的范围。</p><p><img src="https://picturebed-of-hwblog.oss-cn-hangzhou.aliyuncs.com/img/15.png" alt="" title="方法中的一些失败案例"></p><p>致谢：感谢Aaron Hertzmann，Shiry Ginosar，Deepak Pathak，Bryan Russell，Eli Shechtman，Richard Zhang和Tinghui Zhou的许多有益的评论。这项工作得到了NSF SMA1514512，NSF IIS-1633310，Google Research Award，Intel Corp以及NVIDIA的硬件捐赠支持。JYZ由Facebook Graduate Fellowship支持，TP由三星奖学金支持。大多数用于风格转移的照片在法国由AE拍摄。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 风格迁移 </tag>
            
            <tag> CycleGAN </tag>
            
            <tag> 论文翻译 </tag>
            
            <tag> 无监督 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
